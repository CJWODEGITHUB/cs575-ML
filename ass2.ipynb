{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6vMi_G-de_4",
        "outputId": "4bb2c9e0-9d3c-4ba7-d0cb-4c7ea42eb919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Accuracy: 0.6595\n",
            "Sensitivity: [0.819 0.902 0.62  0.845 0.651 0.007 0.029 0.881 0.9   0.941]\n",
            "Specificity: [0.67630058 0.96989247 0.54577465 0.71008403 0.46202981 1.\n",
            " 0.43283582 0.60466712 0.85959885 0.60866753]\n",
            "F1 Score: [0.74084125 0.93471503 0.58052434 0.7716895  0.54047323 0.01390268\n",
            " 0.05435801 0.71713472 0.87933561 0.73919874]\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "def load_mnist_images(filename):\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "    return data.reshape(-1, 28*28)\n",
        "\n",
        "def load_mnist_labels(filename):\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    return labels\n",
        "\n",
        "train_data_path = 'drive/My Drive/zhang/train-images-idx3-ubyte.gz'\n",
        "train_labels_path = 'drive/My Drive/zhang/train-labels-idx1-ubyte.gz'\n",
        "test_data_path = 'drive/My Drive/zhang/t10k-images-idx3-ubyte.gz'\n",
        "test_labels_path = 'drive/My Drive/zhang/t10k-labels-idx1-ubyte.gz'\n",
        "\n",
        "X_train = load_mnist_images(train_data_path) / 255.\n",
        "y_train = load_mnist_labels(train_labels_path)\n",
        "X_test = load_mnist_images(test_data_path) / 255.\n",
        "y_test = load_mnist_labels(test_labels_path)\n",
        "\n",
        "def softmax(X):\n",
        "    expX = np.exp(X - np.max(X))\n",
        "    return expX / expX.sum(axis=1, keepdims=True)\n",
        "\n",
        "def compute_cost(X, y, theta, num_labels):\n",
        "    m = X.shape[0]\n",
        "    predictions = softmax(np.dot(X, theta))\n",
        "    y_onehot = np.eye(num_labels)[y]\n",
        "    logprobs = -np.log(predictions[range(m), y])\n",
        "    cost = np.sum(logprobs) / m\n",
        "    grad = (1/m) * np.dot(X.T, predictions - y_onehot)\n",
        "    return cost, grad\n",
        "\n",
        "def gradient_descent(X, y, theta, alpha, num_iterations, num_labels):\n",
        "    cost_history = []\n",
        "    for i in range(num_iterations):\n",
        "        cost, grad = compute_cost(X, y, theta, num_labels)\n",
        "        theta -= alpha * grad\n",
        "        cost_history.append(cost)\n",
        "    return theta, cost_history\n",
        "\n",
        "m = X_train.shape[0]\n",
        "X_train_bias = np.hstack((np.ones((m, 1)), X_train))\n",
        "num_labels = 10\n",
        "theta = np.zeros((X_train_bias.shape[1], num_labels))\n",
        "theta, cost_history = gradient_descent(X_train_bias, y_train, theta, 0.001, 1200, num_labels)\n",
        "\n",
        "\n",
        "def predict(X, theta):\n",
        "    probs = softmax(np.dot(X, theta))\n",
        "    return np.argmax(probs, axis=1)\n",
        "\n",
        "def confusion_matrix(y_true, y_pred):\n",
        "    num_classes = len(np.unique(y_true))\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for i in range(len(y_true)):\n",
        "        cm[y_true[i]][y_pred[i]] += 1\n",
        "    return cm\n",
        "\n",
        "def metrics_from_cm(cm):\n",
        "    tn = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tn\n",
        "    fn = cm.sum(axis=1) - tn\n",
        "    tp = tn\n",
        "\n",
        "    accuracy = tp.sum() / cm.sum()\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tp / (tp + fp)\n",
        "    f1_score = 2*tp / (2*tp + fp + fn)\n",
        "\n",
        "    return accuracy, sensitivity, specificity, f1_score\n",
        "\n",
        "m_test = X_test.shape[0]\n",
        "X_test_bias = np.hstack((np.ones((m_test, 1)), X_test))\n",
        "y_pred = predict(X_test_bias, theta)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "accuracy, sensitivity, specificity, f1 = metrics_from_cm(cm)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Sensitivity:\", sensitivity)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"F1 Score:\", f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LZTuiryHNKf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Handling\n",
        "Where Data Came From: I took the dataset from Google Drive.\n",
        "How I Loaded Data: I used Python's gzip module and Numpy to get the MNIST Fashion images and labels.\n",
        "Data Look: I changed the images into 784 dimensions (28x28) lines. The labels have 10 types.\n",
        "Which Model I Used\n",
        "Kind of Model: I picked a simple line method for this many-type classification problem.\n",
        "How I Did It: I used Softmax Regression because it's good for many-type classification.\n",
        "How I Made the Model Learn\n",
        "Which Loss Function: I used Cross-entropy loss.\n",
        "How I Improved It: I made it better with Gradient Descent.\n",
        "How Many Times I Tried: I tried 1200 times.\n",
        "How Fast I Made Changes: I set the speed at 0.001.\n",
        "How the Model Did\n",
        "How Right It Was: The model did okay. It got right 0.6595 times out of 1.\n",
        "Sensitivity: For some types like 5 and 6, the model didn't do well.\n",
        "Specificity: For type 5, the model did very well. It didn't say any wrong ones were right.\n",
        "F1 Score: Like Sensitivity, the model didn't do well for some types.\n",
        "My Thoughts\n",
        "The line method, Softmax Regression maybe it's not the best for complicated data like MNIST Fashion.\n",
        "Maybe using more complicated models like CNNs would be better for MNIST Fashion.\n",
        "In the future, I can try changing some settings or use better ways to prepare the data to make the model do better.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-rBHwrDrNQS3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-zCD4jYzNO-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the MNIST Fashion dataset, Softmax Regression might not be the best choice. Maybe using something like Convolutional Neural Networks (CNNs) is better. In the future, I think I can try to change some settings, maybe add some other techniques, or prepare the data in different ways to make the results better."
      ],
      "metadata": {
        "id": "Mb1NIhL3N0Gl"
      }
    }
  ]
}