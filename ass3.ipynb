{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels_onehot = to_categorical(train_labels)\n",
        "test_labels_onehot = to_categorical(test_labels)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.weights1 = np.random.randn(input_size, hidden_size)a\n",
        "        self.weights2 = np.random.randn(hidden_size, output_size)\n",
        "        self.bias1 = np.random.randn(hidden_size)\n",
        "        self.bias2 = np.random.randn(output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.z1 = x.dot(self.weights1) + self.bias1\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "        self.z2 = self.a1.dot(self.weights2) + self.bias2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_prime(self, x):\n",
        "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
        "\n",
        "    def compute_loss(self, y, y_pred):\n",
        "        return -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "\n",
        "    def backward(self, x, y, learning_rate=0.01):\n",
        "        delta2 = (self.a2 - y) * self.sigmoid_prime(self.z2)\n",
        "        delta1 = delta2.dot(self.weights2.T) * self.sigmoid_prime(self.z1)\n",
        "\n",
        "        self.weights2 -= learning_rate * self.a1.T.dot(delta2)\n",
        "        self.bias2 -= learning_rate * np.sum(delta2, axis=0)\n",
        "\n",
        "        self.weights1 -= learning_rate * x.T.dot(delta1)\n",
        "        self.bias1 -= learning_rate * np.sum(delta1, axis=0)\n",
        "\n",
        "def compute_confusion_matrix(true, pred):\n",
        "    K = len(np.unique(true))\n",
        "    result = np.zeros((K, K))\n",
        "    for i in range(len(true)):\n",
        "        result[true[i]][pred[i]] += 1\n",
        "    return result\n",
        "\n",
        "def cross_validation(images, labels, k=5):\n",
        "    fold_size = len(images) // k\n",
        "    for fold in range(k):\n",
        "        val_start = fold * fold_size\n",
        "        val_end = (fold + 1) * fold_size\n",
        "\n",
        "        x_val = images[val_start:val_end]\n",
        "        y_val = labels[val_start:val_end]\n",
        "\n",
        "        x_train = np.vstack([images[:val_start], images[val_end:]])\n",
        "        y_train = np.vstack([labels[:val_start], labels[val_end:]])\n",
        "\n",
        "        yield x_train, y_train, x_val, y_val\n",
        "\n",
        "all_accuracies = []\n",
        "all_sensitivities = []\n",
        "all_specificities = []\n",
        "all_f1_scores = []\n",
        "\n",
        "for x_train, y_train, x_val, y_val in cross_validation(train_images, train_labels_onehot):\n",
        "\n",
        "    nn = NeuralNetwork(input_size=28*28, hidden_size=64, output_size=10)\n",
        "\n",
        "    epochs = 20\n",
        "    batch_size = 8\n",
        "    learning_rate = 0.01\n",
        "    num_batches = x_train.shape[0] // batch_size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for i in range(num_batches):\n",
        "            x_batch = x_train[i*batch_size:(i+1)*batch_size]\n",
        "            y_batch = y_train[i*batch_size:(i+1)*batch_size]\n",
        "\n",
        "            y_pred = nn.forward(x_batch)\n",
        "            total_loss += nn.compute_loss(y_batch, y_pred)\n",
        "\n",
        "            nn.backward(x_batch, y_batch, learning_rate)\n",
        "\n",
        "        average_loss = total_loss / num_batches\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "    y_val_pred = nn.forward(x_val)\n",
        "    predicted_labels = np.argmax(y_val_pred, axis=1)\n",
        "    true_labels = np.argmax(y_val, axis=1)\n",
        "\n",
        "    confusion = compute_confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "    FP = confusion.sum(axis=0) - np.diag(confusion)\n",
        "    FN = confusion.sum(axis=1) - np.diag(confusion)\n",
        "    TP = np.diag(confusion)\n",
        "    TN = confusion.sum() - (FP + FN + TP)\n",
        "\n",
        "    accuracy = np.sum(TP) / np.sum(confusion)\n",
        "    sensitivity = TP / (TP + FN)\n",
        "    specificity = TN / (TN + FP)\n",
        "\n",
        "    precision = TP / (TP + FP + 1e-7)\n",
        "    recall = TP / (TP + FN + 1e-7)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
        "\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_sensitivities.append(np.mean(sensitivity))\n",
        "    all_specificities.append(np.mean(specificity))\n",
        "    all_f1_scores.append(np.mean(f1))\n",
        "\n",
        "print(\"Average Accuracy:\", np.mean(all_accuracies))\n",
        "print(\"Average Sensitivity:\", np.mean(all_sensitivities))\n",
        "print(\"Average Specificity:\", np.mean(all_specificities))\n",
        "print(\"Average F1 Score:\", np.mean(all_f1_scores))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9paRMhmT6mfW",
        "outputId": "16fc8f51-a019-4916-c9f2-a043f45220fe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 42.7375\n",
            "Epoch 2/20, Loss: 24.4986\n",
            "Epoch 3/20, Loss: 19.2521\n",
            "Epoch 4/20, Loss: 17.6841\n",
            "Epoch 5/20, Loss: 16.9413\n",
            "Epoch 6/20, Loss: 16.4872\n",
            "Epoch 7/20, Loss: 16.1611\n",
            "Epoch 8/20, Loss: 15.9048\n",
            "Epoch 9/20, Loss: 15.6963\n",
            "Epoch 10/20, Loss: 15.5237\n",
            "Epoch 11/20, Loss: 15.3768\n",
            "Epoch 12/20, Loss: 15.2492\n",
            "Epoch 13/20, Loss: 15.1389\n",
            "Epoch 14/20, Loss: 15.0441\n",
            "Epoch 15/20, Loss: 14.9622\n",
            "Epoch 16/20, Loss: 14.8904\n",
            "Epoch 17/20, Loss: 14.8265\n",
            "Epoch 18/20, Loss: 14.7686\n",
            "Epoch 19/20, Loss: 14.7154\n",
            "Epoch 20/20, Loss: 14.6660\n",
            "Epoch 1/20, Loss: 31.9864\n",
            "Epoch 2/20, Loss: 18.2768\n",
            "Epoch 3/20, Loss: 12.6674\n",
            "Epoch 4/20, Loss: 10.2708\n",
            "Epoch 5/20, Loss: 9.0076\n",
            "Epoch 6/20, Loss: 8.2566\n",
            "Epoch 7/20, Loss: 7.7191\n",
            "Epoch 8/20, Loss: 7.2915\n",
            "Epoch 9/20, Loss: 6.9299\n",
            "Epoch 10/20, Loss: 6.6191\n",
            "Epoch 11/20, Loss: 6.3485\n",
            "Epoch 12/20, Loss: 6.1122\n",
            "Epoch 13/20, Loss: 5.9062\n",
            "Epoch 14/20, Loss: 5.7238\n",
            "Epoch 15/20, Loss: 5.5581\n",
            "Epoch 16/20, Loss: 5.4049\n",
            "Epoch 17/20, Loss: 5.2625\n",
            "Epoch 18/20, Loss: 5.1326\n",
            "Epoch 19/20, Loss: 5.0154\n",
            "Epoch 20/20, Loss: 4.9092\n",
            "Epoch 1/20, Loss: 50.7605\n",
            "Epoch 2/20, Loss: 41.6897\n",
            "Epoch 3/20, Loss: 35.6545\n",
            "Epoch 4/20, Loss: 33.1615\n",
            "Epoch 5/20, Loss: 31.9469\n",
            "Epoch 6/20, Loss: 30.8884\n",
            "Epoch 7/20, Loss: 29.7089\n",
            "Epoch 8/20, Loss: 28.3481\n",
            "Epoch 9/20, Loss: 27.0658\n",
            "Epoch 10/20, Loss: 25.9835\n",
            "Epoch 11/20, Loss: 24.9969\n",
            "Epoch 12/20, Loss: 23.9747\n",
            "Epoch 13/20, Loss: 22.8643\n",
            "Epoch 14/20, Loss: 21.5491\n",
            "Epoch 15/20, Loss: 19.9583\n",
            "Epoch 16/20, Loss: 18.4364\n",
            "Epoch 17/20, Loss: 15.7297\n",
            "Epoch 18/20, Loss: 10.9619\n",
            "Epoch 19/20, Loss: 8.6456\n",
            "Epoch 20/20, Loss: 6.3530\n",
            "Epoch 1/20, Loss: 48.8389\n",
            "Epoch 2/20, Loss: 37.0713\n",
            "Epoch 3/20, Loss: 28.2013\n",
            "Epoch 4/20, Loss: 23.7093\n",
            "Epoch 5/20, Loss: 20.8334\n",
            "Epoch 6/20, Loss: 16.8601\n",
            "Epoch 7/20, Loss: 15.6536\n",
            "Epoch 8/20, Loss: 15.1896\n",
            "Epoch 9/20, Loss: 14.9128\n",
            "Epoch 10/20, Loss: 14.7214\n",
            "Epoch 11/20, Loss: 14.5734\n",
            "Epoch 12/20, Loss: 14.4511\n",
            "Epoch 13/20, Loss: 14.3467\n",
            "Epoch 14/20, Loss: 14.2561\n",
            "Epoch 15/20, Loss: 14.1762\n",
            "Epoch 16/20, Loss: 14.1042\n",
            "Epoch 17/20, Loss: 14.0380\n",
            "Epoch 18/20, Loss: 13.9763\n",
            "Epoch 19/20, Loss: 13.9177\n",
            "Epoch 20/20, Loss: 13.8612\n",
            "Epoch 1/20, Loss: 59.9007\n",
            "Epoch 2/20, Loss: 52.3924\n",
            "Epoch 3/20, Loss: 42.7688\n",
            "Epoch 4/20, Loss: 36.2657\n",
            "Epoch 5/20, Loss: 33.8326\n",
            "Epoch 6/20, Loss: 32.0183\n",
            "Epoch 7/20, Loss: 28.8882\n",
            "Epoch 8/20, Loss: 27.0435\n",
            "Epoch 9/20, Loss: 26.0118\n",
            "Epoch 10/20, Loss: 24.9191\n",
            "Epoch 11/20, Loss: 24.0921\n",
            "Epoch 12/20, Loss: 23.4700\n",
            "Epoch 13/20, Loss: 22.8372\n",
            "Epoch 14/20, Loss: 22.0291\n",
            "Epoch 15/20, Loss: 21.0549\n",
            "Epoch 16/20, Loss: 20.0265\n",
            "Epoch 17/20, Loss: 19.2164\n",
            "Epoch 18/20, Loss: 18.6937\n",
            "Epoch 19/20, Loss: 18.3326\n",
            "Epoch 20/20, Loss: 18.0731\n",
            "Average Accuracy: 0.85015\n",
            "Average Sensitivity: 0.8482209122446889\n",
            "Average Specificity: 0.9833654901134248\n",
            "Average F1 Score: 0.8237090384870835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Issue We Want to Solve\n",
        "We hope to build a simple feed-forward neural network. The main purpose is to classify handwritten numbers in the MNIST dataset. Our aim is to predict which number from 0 to 9 the given handwritten image represents.\n",
        "\n",
        "2. Data Information\n",
        "\n",
        "Data Source: We are using the MNIST dataset from the Keras library. This dataset is a very famous set for hand-written digits.\n",
        "Data Type and Size: It has grayscale images of hand-written numbers. For training, 60,000 images are provided, and for testing, there are 10,000 images.\n",
        "Data Quality: All images are centered and bounded to fit in a 28x28 pixel box.\n",
        "3. Data Preparation Steps\n",
        "\n",
        "Reshaping Images: Each image from the dataset, originally in a 28x28 matrix form, is flattened into a vector of length 784.\n",
        "Normalization: Pixel values are originally between 0 to 255. We scale these values between 0 to 1 by dividing with 255. This step can make training faster and more stable.\n",
        "One-hot Encoding: The original labels, which are integers from 0 to 9, are transformed into one-hot encoded vectors. This means, for example, number 2 is represented as [0, 0, 1, 0, 0, 0, 0, 0, 0, 0].\n",
        "4. How Neural Network is Built\n",
        "Our neural network is quite basic but effective:\n",
        "\n",
        "Input Layer: Takes in a vector of size 784, which comes from our 28x28 pixel images.\n",
        "Hidden Layer: This is a middle layer of 64 neurons. Sigmoid function is used here for activation. It helps to introduce non-linearity to the model.\n",
        "Output Layer: 10 neurons are here, matching the 10 possible digits. Each neuron predicts the probability of a particular number.\n",
        "5. Training the Neural Network\n",
        "\n",
        "Loss Function: We are using negative log-likelihood. It measures the difference between our predicted probabilities and the actual labels.\n",
        "Backpropagation: After forwarding data through the network, we use backpropagation to update the weights and biases. This algorithm calculates the gradient of the loss function concerning each weight by the chain rule.\n",
        "Learning Rate: Set at 0.01, it controls how much we adjust the weights in response to the calculated error.\n",
        "6. About Cross-validation\n",
        "We don’t want to overfit, so we use k-fold cross-validation (k=5). It means we split our training data into 5 subsets. We train on 4 and validate on 1, then rotate and repeat. This helps in getting a better understanding of the model's performance.\n",
        "\n",
        "7. Checking Model's Performance\n",
        "After training, on the validation set, we measure:\n",
        "\n",
        "Accuracy: Percentage of correctly predicted instances.\n",
        "Sensitivity (Recall): The ability of our model to identify positive classes correctly.\n",
        "Specificity: The ability of the classifier to find out the negative classes.\n",
        "F1 Score: A balance between precision and recall. Higher is better.\n",
        "8. Final Words on Results\n",
        "Our neural network, with the settings and parameters given, has been trained on the MNIST dataset. After i test, the 20,8,0.1 is the best parameter for this project, the batch size and learning rate which if i set up too high, it will reduce the accuacy of this model. Future work can look into improving performance by changing architecture, introducing more advanced techniques, or optimizing parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "4GNbwI3t9dkf"
      }
    }
  ]
}