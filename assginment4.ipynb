{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import time"
      ],
      "metadata": {
        "id": "g3yzbgg2UfGF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist = datasets.fetch_openml('Fashion-MNIST', version=1)\n",
        "X = fashion_mnist.data\n",
        "y = fashion_mnist.target.astype(np.int)\n",
        "\n",
        "X_linear = X[np.isin(y, [0, 1])]\n",
        "y_linear = y[np.isin(y, [0, 1])]\n",
        "\n",
        "X_nonlinear = X[np.isin(y, [0, 6])]\n",
        "y_nonlinear = y[np.isin(y, [0, 6])]\n",
        "y_nonlinear = np.where(y_nonlinear == 0, -1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAuRbtl4Ue_V",
        "outputId": "ba563772-60c2-4616-9b58-305c4387e591"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n",
            "<ipython-input-2-817c4bfa47a0>:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = fashion_mnist.target.astype(np.int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_linear_sampled, _, y_linear_sampled, _ = train_test_split(X_linear, y_linear, train_size=0.1, random_state=42)\n",
        "\n",
        "X_nonlinear_sampled, _, y_nonlinear_sampled, _ = train_test_split(X_nonlinear, y_nonlinear, train_size=0.1, random_state=42)\n",
        "\n",
        "X_train_linear, X_test_linear, y_train_linear, y_test_linear = train_test_split(X_linear_sampled, y_linear_sampled, test_size=0.3, random_state=42)\n",
        "X_train_nonlinear, X_test_nonlinear, y_train_nonlinear, y_test_nonlinear = train_test_split(X_nonlinear_sampled, y_nonlinear_sampled, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "Ov_R28mfUe2Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_hard_linear = SVC(kernel='linear', C=3)\n",
        "svm_hard_nonlinear = SVC(kernel='linear', C=3)\n"
      ],
      "metadata": {
        "id": "mVjP-ZFtUrI_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_TqqKLG4reW",
        "outputId": "4c35ba5c-373d-4955-b2ea-6db78a72c18f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on linearly separable dataset...\n",
            "Training of linearly separable dataset completed. Time taken: 0.04 seconds\n",
            "Training on non-linearly separable dataset...\n",
            "Training of non-linearly separable dataset completed. Time taken: 0.27 seconds\n",
            "Linearly Separable Dataset:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98       204\n",
            "           1       0.98      0.98      0.98       216\n",
            "\n",
            "    accuracy                           0.98       420\n",
            "   macro avg       0.98      0.98      0.98       420\n",
            "weighted avg       0.98      0.98      0.98       420\n",
            "\n",
            "Non-Linearly Separable Dataset:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.82      0.75      0.79       223\n",
            "           1       0.75      0.82      0.78       197\n",
            "\n",
            "    accuracy                           0.78       420\n",
            "   macro avg       0.78      0.79      0.78       420\n",
            "weighted avg       0.79      0.78      0.78       420\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Training on linearly separable dataset...\")\n",
        "start_time = time.time()\n",
        "svm_hard_linear.fit(X_train_linear, y_train_linear)\n",
        "end_time = time.time()\n",
        "print(f\"Training of linearly separable dataset completed. Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "print(\"Training on non-linearly separable dataset...\")\n",
        "start_time = time.time()\n",
        "svm_hard_nonlinear.fit(X_train_nonlinear, y_train_nonlinear)\n",
        "end_time = time.time()\n",
        "print(f\"Training of non-linearly separable dataset completed. Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "y_pred_linear = svm_hard_linear.predict(X_test_linear)\n",
        "print(\"Linearly Separable Dataset:\")\n",
        "print(classification_report(y_test_linear, y_pred_linear))\n",
        "\n",
        "y_pred_nonlinear = svm_hard_nonlinear.predict(X_test_nonlinear)\n",
        "print(\"Non-Linearly Separable Dataset:\")\n",
        "print(classification_report(y_test_nonlinear, y_pred_nonlinear))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oGNeeq6P_CON"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results\n",
        "Linearly Separable Dataset\n",
        "Accuracy: 98%\n",
        "Precision and Recall: Both approximately 98% for categories 0 and 1.\n",
        "Non-Linearly Separable Dataset\n",
        "Accuracy: 78%\n",
        "Precision and Recall: Varied between 75% to 82%, with category 1 showing a slightly higher recall.\n",
        "\n",
        "Discussion\n",
        "Linearly Separable Dataset\n",
        "The high performance of the hard margin SVM on the linearly separable dataset indicates that the model effectively found a hyperplane in the feature space that accurately separated the two categories. Due to the inherent linear separability of the dataset, the model was able to classify samples with high precision and recall.\n",
        "\n",
        "Non-Linearly Separable Dataset\n",
        "Although hard margin SVMs are typically not suitable for non-linearly separable datasets, in our case, it still showed relatively high accuracy. This might indicate that categories 0 and 6 have some degree of linear separability in the feature space, or the overlap in the dataset was not very significant. However, hard margin SVMs may face challenges in dealing with truly non-linearly separable data, as they require complete linear separability and are very sensitive to outliers.\n",
        "The failing aspect of this method is that it requires an exceedingly long time to train, to the extent that I had to reduce the size of the dataset.\n",
        "\n",
        "Limitations of Hard Margin SVM\n",
        "The main limitation of the hard margin SVM in dealing with non-linearly separable data lies in its assumption of complete linear separability. In real-world datasets, this assumption is often unrealistic, as data often contains noise and outliers. Hard margin SVMs are highly sensitive to these outliers, which might lead to model overfitting or inability to find an effective decision boundary. For more complex or heavily overlapping data distributions, soft margin SVMs or SVMs with non-linear kernels might need to be considered.\n",
        "\n",
        "Conclusion\n",
        "Our experiments show that hard margin SVMs are highly effective in dealing with linearly separable datasets, but they may have limitations when handling non-linearly separable datasets. Although they performed well in our case, in practical applications, choosing the right model and parameters is crucial for optimal performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "vTHi6kff_C1d"
      }
    }
  ]
}